{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a YouTube Transcript Summarizer with LangChain and OpenRouter\n",
    "\n",
    "This tutorial walks through building a system to extract, process and summarize YouTube video transcripts using LangChain, OpenRouter (with Google's Gemini model), and semantic text chunking.\n",
    "\n",
    "The final output will be a concise summary of the video with key takeaways, processed through semantic chunking and refined using the LLM.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "1. **Semantic Chunking**: Instead of simple text splitting, uses semantic understanding to create meaningful chunks\n",
    "2. **Custom Prompts**: Uses separate prompts for initial summary and refinement\n",
    "3. **Refinement Chain**: Processes chunks sequentially, refining the summary with each new piece of context\n",
    "4. **OpenRouter Integration**: Leverages Google's Gemini model through OpenRouter's API\n",
    "\n",
    "This system provides a sophisticated way to generate meaningful summaries from YouTube video transcripts, making it easier to extract key information from long-form content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from typing import Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_core.utils.utils import secret_from_env\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import Field, SecretStr\n",
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: YouTube Transcript Extraction\n",
    "Then, we create functions to extract video transcripts from YouTube URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_id(url):\n",
    "    \"\"\"\n",
    "    Extract YouTube video ID from URL.\n",
    "    Parameters:\n",
    "        url (str): The YouTube URL to extract the video ID from.\n",
    "    Returns:\n",
    "        str: The extracted video ID, or None if no valid ID is found.\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*',\n",
    "        r'(?:youtu\\.be\\/)([0-9A-Za-z_-]{11})'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, url)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return None\n",
    "\n",
    "def get_youtube_transcript(url):\n",
    "    \"\"\"\n",
    "    Get the transcript of a YouTube video.\n",
    "    Parameters:\n",
    "        url (str): The YouTube URL to get the transcript from.\n",
    "    Returns:\n",
    "        str: The transcript of the video, or an error message if the URL is invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        video_id = extract_video_id(url)\n",
    "        if not video_id:\n",
    "            raise ValueError(\"Invalid YouTube URL\")\n",
    "            \n",
    "        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        full_transcript = ' '.join(entry['text'] for entry in transcript_list)\n",
    "        return full_transcript #count\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_url = 'https://www.youtube.com/watch?v=q4DQaMtHvsI&ab_channel=InstituteofPolicyStudies%28IPS%29%2CSingapore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = get_youtube_transcript(youtube_url)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Text Embedding with Stella Model\n",
    "We use the Stella language model for text embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://huggingface.co/NovaSearch/stella_en_400M_v5/blob/main/README.md\n",
    "# Define the model name and configuration\n",
    "model_name = \"dunzhang/stella_en_400M_v5\"\n",
    "model_kwargs = {\n",
    "    'trust_remote_code': True,\n",
    "    'device': 'cpu',\n",
    "    'config_kwargs': {\n",
    "        'use_memory_efficient_attention': False,\n",
    "        'unpad_inputs': False\n",
    "    }\n",
    "}\n",
    "encode_kwargs = {\n",
    "    'normalize_embeddings': False\n",
    "}\n",
    "\n",
    "# Initialize the HuggingFaceEmbeddings with the Stella model\n",
    "stella_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs)\n",
    "document_embeddings = stella_embeddings.embed_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.langchain.com/api_reference/experimental/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html\n",
    "chunker = SemanticChunker(embeddings=stella_embeddings)\n",
    "\n",
    "# SSplit text into chunks and convert to Documents\n",
    "chunks = chunker.split_text(text)\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i + 1}:\")\n",
    "    pprint(doc.page_content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setting up OpenRouter with Gemini\n",
    "We create a custom ChatOpenRouter class to use OpenRouter's API with Google's Gemini model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# https://openrouter.ai/google/gemini-2.0-flash-exp:free/api\n",
    "class ChatOpenRouter(ChatOpenAI):\n",
    "    openai_api_key: Optional[SecretStr] = Field(\n",
    "        alias=\"api_key\",\n",
    "        default_factory=secret_from_env(\"OPENROUTER_API_KEY\", default=None),\n",
    "    )\n",
    "    @property\n",
    "    def lc_secrets(self) -> dict[str, str]:\n",
    "        return {\"openai_api_key\": \"OPENROUTER_API_KEY\"}\n",
    "\n",
    "    def __init__(self,\n",
    "                 openai_api_key: Optional[str] = None,\n",
    "                 **kwargs):\n",
    "        openai_api_key = (\n",
    "            openai_api_key or os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "        )\n",
    "        super().__init__(\n",
    "            base_url=\"https://openrouter.ai/api/v1\",\n",
    "            openai_api_key=openai_api_key,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "llm = ChatOpenRouter(\n",
    "    model_name=\"google/gemini-2.0-flash-exp:free\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prompting the LLM for Summarization Chain with Refinement\n",
    "We use the refine summarization chain to generate a summary of the video transcript:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference https://medium.com/the-data-perspectives/custom-prompts-for-langchain-chains-a780b490c199\n",
    "question_template = PromptTemplate.from_template(\"\"\"\n",
    "Write a concise summary of the following youtube transcript with key takeaways for the audience:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\")\n",
    "\n",
    "refine_template = PromptTemplate.from_template(\"\"\"Your job is to produce a final key takeaways summary.\n",
    "We have provided an existing summary up to a certain point: {existing_answer}\n",
    "\n",
    "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "Given the new context, refine the original summary with new key takeaways.\n",
    "If the context isn't useful, return the original summary.\\\n",
    "\"\"\")\n",
    "\n",
    "# Load the refine summarization chain\n",
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=question_template,\n",
    "    refine_prompt = refine_template,\n",
    "    verbose=True,\n",
    "    document_variable_name=\"text\", \n",
    "    initial_response_name=\"existing_answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Putting It All Together\n",
    "Here's how to use the complete system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_comb = chain.invoke(documents)\n",
    "pprint(output_comb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
